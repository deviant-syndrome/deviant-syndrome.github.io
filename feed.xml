<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deviant Syndrome</title>
    <description>coding, multimedia, gamedev, engineering</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 21 Jul 2023 20:28:27 +0300</pubDate>
    <lastBuildDate>Fri, 21 Jul 2023 20:28:27 +0300</lastBuildDate>
    <generator>Jekyll v4.3.2</generator>
    
      <item>
        <title>Three Random Facts</title>
        <description>&lt;p&gt;Unrelated to anything, but I just wanted to share three random encounters with some seriously weird stuff.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;XCode, by default, if you’re running your code it will pause the execution every time it stumbles 
upon a failing assertion. Annoying as hell. The fix for that is somewhat brutal.
    &lt;ul&gt;
      &lt;li&gt;Open the “Breakpoint Navigator” by clicking on the icon that looks like a right-angle or arrow in the left pane of the Xcode window.&lt;/li&gt;
      &lt;li&gt;In the bottom left of the “Breakpoint Navigator”, click on the ‘+’ sign, then select “Exception Breakpoint…”&lt;/li&gt;
      &lt;li&gt;A new window will open. Choose “All” for “Exception”&lt;/li&gt;
      &lt;li&gt;Check “Automatically continue after evaluating actions”! That’s it&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Ever trying to reset some MacOS application settings. Like, there configuration files are nowhere to be found.
It’s all because of Objective-C NSUserDefaults. They are supposed to be stored in ~/Library/Preferences/ folder.
However, only executing the following made the settings reset for good:
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;defaults delete com.codeplex.pcsxr
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;com.codeplex.pcsxr&lt;/code&gt; is just an example, but all the other application namespaces are similar.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;In SLADE you have to add the scripts manually to the WAD archive, otherwise they won’t be executed. To make this work with UDMF, you have to add the scripts LUMP before the ENDMAP marker. Also, you need recomplile it manually if changed.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 08 Jul 2023 18:55:01 +0300</pubDate>
        <link>/2023/Three_Random_Facts/</link>
        <guid isPermaLink="true">/2023/Three_Random_Facts/</guid>
        
        <category>xcode</category>
        
        
        <category>code bits</category>
        
      </item>
    
      <item>
        <title>Importing WaveEdit Wavetables to Pigments</title>
        <description>&lt;p&gt;Just a quick note on how to import wavetables from &lt;a href=&quot;https://www.waveeditonline.com/&quot;&gt;WaveEdit&lt;/a&gt; to &lt;a href=&quot;https://www.arturia.com/products/analog-classics/pigments&quot;&gt;Pigments&lt;/a&gt;
Pigments is an brilliant exhibit of what is now called a &lt;em&gt;power synth&lt;/em&gt;-buzzword, I first encountered it in “Future Music” magazine  Much like &lt;a href=&quot;https://www.native-instruments.com/en/products/komplete/synths/massive/&quot;&gt;Massive&lt;/a&gt;, 
it combines a pack of different synthesis techniques (engines), all streamlined into a single interface for you to mix and match. Both of aforementioned products
also sport an easy “drag-and-drop” modulation system, which is a great way to get started with sound design.&lt;/p&gt;

&lt;p&gt;Pigments is a great synth, but it does not have a built-in wavetable editor. So, how about another 
weird audio-software-combo? I’m not sure if it is a good idea, but it is definitely a fun one. 
I think that WaveEdit was originally designed to be used with &lt;a href=&quot;https://www.openacousticdevices.info/&quot;&gt;AudioMoth&lt;/a&gt;, or some
other audio hardware, but it is also a great tool for wavetable editing with a very appealing UI. 
More importantly, it has an integrated library of community-curated wavetables, which you can use as a starting point for your own designs,
or to approximate some of the classic digital synth sounds, like the ones from &lt;a href=&quot;https://waldorfmusic.com/en/microwave&quot;&gt;Waldorf Microwave&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/PPG_Wave&quot;&gt;PPG Wave&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There is a tiny compatibility issue, though, probably related to wavetable resolution. If I save a wavetable in WaveEdit, and then try to import it to Pigments, the 
frames do not line up. The wavetable is still usable, but it is not exactly what I wanted. I think that the problem is that WaveEdit uses 64 frames per wavetable, while Pigments uses 512.
How do we make them align properly? Simple! Just make it 8x slower, e.g. apply the speed multiplier of 0.125 and you are good to go.&lt;/p&gt;

</description>
        <pubDate>Fri, 23 Jun 2023 20:37:35 +0300</pubDate>
        <link>/2023/Importing_WaveEdit_Wavetables_to_Pigments/</link>
        <guid isPermaLink="true">/2023/Importing_WaveEdit_Wavetables_to_Pigments/</guid>
        
        <category>waveedit</category>
        
        <category>software</category>
        
        
        <category>dsp</category>
        
      </item>
    
      <item>
        <title>SSH Agent under MacOS Ventura</title>
        <description>&lt;p&gt;After upgrading to MacOS Venture I had a bit of a problem with the SSH agent. This problem, though, could still 
be related to the interface between the monitor and the chair, as we developers like to say.&lt;/p&gt;

&lt;p&gt;I’m using multiple SSH keys, mainly to distinguish between my personal and work GitHub accounts. Git has a nice option for that, btw:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;GIT_SSH_COMMAND&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'ssh -i ~/.ssh/personal_github_rsa -o IdentitiesOnly=yes'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After the upgrade, 
I migrated the SSH keys from the old machine. Manually. I mean, like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp&lt;/code&gt;-manually. Don’t ask me why I did not use the standard migration tools 
. Believe me, that are something things behind those slick silver facade, that you don’t want to know about. But to imagine the 
horror I will just say, that the experience with migration tools for my platform made me really miss &lt;a href=&quot;https://clonezilla.org/&quot;&gt;CloneZilla&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, back to the topic. After the migration, when I tried to push something to my personal GitHub repo, I got the authentication error.
So, I had to add the key to the agent. I did it like this:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;ssh-agent &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
ssh-add &lt;span class=&quot;nt&quot;&gt;-K&lt;/span&gt; ~/.ssh/personal_github_rsa
ssh-add &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-K&lt;/code&gt; option was specific to macOS and adds the specified identity file to the Keychain, which is the secure password management system on macOS.&lt;/p&gt;

&lt;p&gt;Lately, I also stumbled upon this:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;WARNING: The -K and -A flags are deprecated and have been replaced
by the --apple-use-keychain and --apple-load-keychain
flags, respectively.  To suppress this warning, set the
environment variable APPLE_SSH_ADD_BEHAVIOR as described in
the ssh-add(1) manual page.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;So, probably, it is better to use the new options:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh-add &lt;span class=&quot;nt&quot;&gt;--apple-use-keychain&lt;/span&gt; ~/.ssh/personal_github_rsa
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Sat, 17 Jun 2023 12:15:38 +0300</pubDate>
        <link>/2023/SSH_Agent_under_MacOS_Ventura/</link>
        <guid isPermaLink="true">/2023/SSH_Agent_under_MacOS_Ventura/</guid>
        
        <category>bash</category>
        
        
        <category>software</category>
        
      </item>
    
      <item>
        <title>WaveEdit and Virtual Outputs</title>
        <description>&lt;p&gt;This will be a short one. I just wanted to share a quick tip on how to use &lt;a href=&quot;https://www.waveeditonline.com/&quot;&gt;WaveEdit&lt;/a&gt; with virtual audio outputs, 
if case someone is bashing their head against the wall trying to figure out what is wrong with it.&lt;/p&gt;

&lt;p&gt;Earlier I reported, that Audacity had some problems with recording virtual audio outputs. I tried to record 
WaveEdit output with it, using BlackHole 16ch, and it was not working. I was getting a blank recording. That was a bit strange, because 
I was able to record WaveEdit output with other applications, like &lt;a href=&quot;https://sox.sourceforge.net/&quot;&gt;SoX&lt;/a&gt; and &lt;a href=&quot;https://ffmpeg.org/ffplay.html&quot;&gt;ffplay&lt;/a&gt;.
BlackHole 16ch also worked flawlessly for all the other applications and use-cases I had. So, I was a bit puzzled, 
until I checked the multi-channel wave rendered by sox. It was channel 3 and channel 4, that had the actual audio data, Not 1 and 2, as one might expect.
Why does WaveEdit (or is it sox?) act like this is beyond my comprehension.&lt;/p&gt;

</description>
        <pubDate>Sat, 17 Jun 2023 11:47:55 +0300</pubDate>
        <link>/2023/WaveEdit_and_Virtual_Outputs/</link>
        <guid isPermaLink="true">/2023/WaveEdit_and_Virtual_Outputs/</guid>
        
        <category>waveedit</category>
        
        <category>software</category>
        
        
        <category>dsp</category>
        
      </item>
    
      <item>
        <title>History of Digital Recording</title>
        <description>&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/a/a9/Dat_cartridge.jpg&quot; alt=&quot;heck&quot; width=&quot;100%&quot; style=&quot;margin-top: 20px;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;at-least-my-take-on-it&quot;&gt;At Least, My Take On It&lt;/h4&gt;

&lt;p&gt;So, during another lunch break at my not-creative-at-all day job, we were discussing the possibility of existence of purely digital recordings in the 80’s. 
I always hijack the conversation to talk about music, and this time was no exception, which, of course, tells you a lot about my social skills and level of adaptation.&lt;/p&gt;

&lt;p&gt;I was quite sure that it was possible, but I was not sure about the exact dates and the technology used.
Over the course of discussion, one of my colleagues even challenged me to tighten up the definition of “digital recording” itself.
This was the turning point, because I realized, that we are talking about the existence of systems or solutions that included both tools digital synthesis and sequencing, as well as digital recording the result.
All that extravaganza when the DSP was still at its infancy.
As we all know, 80’s were flourishing with digital synthesizers, but the digital recording was not that widespread, so
you would still track your beloved Yamaha DX7 or Roland D-50 to a tape recorder, or a reel-to-reel machine. 
During the conversation, we agreed, that this does not count as a digital recording, because the recording medium is analog.&lt;/p&gt;

&lt;p&gt;So, I made some vague hypothesis on what kind of setup that might be and what was the year when it hit the market. 
I decided later, that I will do some quick fact-checking to update my friends with the correct answer. This however, turned into 
a full-blown mystery investigation, which I am going to share with you.&lt;/p&gt;

&lt;!-- readmore --&gt;
&lt;h4 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h4&gt;
&lt;p&gt;In this article we will be talking specifically of digital recording and not distribution. This means that the history of CD’s 
will be intentionally left aside. I’ll just mention that most of the CDs produced in the 80’s were mastered from analog tapes.&lt;/p&gt;

&lt;h4 id=&quot;1-1970s&quot;&gt;1. 1970’s&lt;/h4&gt;
&lt;p&gt;Commercially available devices using &lt;a href=&quot;https://en.wikipedia.org/wiki/Digital_recording&quot;&gt;PCM&lt;/a&gt; technology apparently existed since late 1960’s. Digital signal was print onto industrial-grade videotapes. During that time it was primarily
used in broadcasting and mastering of analog records. A few digital recordings of orchestral (mainly jazz) pieces were
made during 1970s.&lt;/p&gt;

&lt;h4 id=&quot;2-1990s&quot;&gt;2. 1990’s&lt;/h4&gt;
&lt;p&gt;It was, however, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Digital_Audio_Tape&quot;&gt;compact-cassette digital format&lt;/a&gt;, first introduced in 1989 by Sony, that was the DAT how most of us
remember it, if we had heard of it at all. Technology was quite widespread in mostly semi-professional and project
studios during 1990s. Then it kinda died out, and the reasons for that, if condensed in one paragraph, would be two main things.
First, were the medium issues, DAT tape was quite thin, and it was prone to stretching and breaking, and unlike analog tape
which degrades gracefully, digital recording is either perfect or broken. Also, DAT playback was much more mechanically complex
because of rotary helical scan head, which was also prone to mechanical failure.
Second is, sadly, less technical and more political. This was something, remembered as “anti-DAT” lobbying, which was a campaign by RIAA, mainly in US.&lt;/p&gt;

&lt;h4 id=&quot;3-wait-what-about-the-80s&quot;&gt;3. Wait, what about the 80’s?&lt;/h4&gt;

&lt;p&gt;No matter how I tried, I could not present the development of digital recording as a chronologically linear process.
In that era, there were at least three technologies of digitall recording, developed independently, each with it’s own
philosophy and approach. I will try to present them in (approximate) order of their appearance.&lt;/p&gt;

&lt;h4 id=&quot;30-csound-1985&quot;&gt;3.0 CSound (~1985)&lt;/h4&gt;
&lt;p&gt;This entry falls more into an “honorably mentioned” category, because this thing is mostly used in academic circles, and
hardly any commercial or even “independent” music was produced using it. During the development of CSound in the 1980s, it was primarily run on mainframe computers, such as the &lt;a href=&quot;https://en.wikipedia.org/wiki/PDP-10&quot;&gt;DEC PDP-10&lt;/a&gt; and CDC Cyber series. These mainframes were commonly found in academic and research institutions, where CSound was initially developed and used.
I could hardly imagine any adequate digital storage media for the CSound output, available around that time.&lt;/p&gt;

&lt;h4 id=&quot;31-ppg-wave-system-1987&quot;&gt;3.1 PPG Wave System (~1987)&lt;/h4&gt;

&lt;p&gt;One the first fully-digital synthesis and recording solutions could be considered PPG Wave System, which dates as far, as approximately 1987.  A multi-component frankenstein put together from a line of earlier PPG hardware was re-using wavetable digital
technology of earlier PPG Synths, WaveComputer, which was developed as a competitor to the monstrous Fairlight CMI,
which had only sampling and sequencing capabilities. A battle between Godzilla and King-Kong, indeed. The system was enormously expensive, and did not get wide adoption. 
It used own proprietary hardware for storage of digital data called &lt;a href=&quot;https://web.archive.org/web/20131208112832/http://theppgs.com/hdu.html&quot;&gt;Hard Disk Unit (HDU)&lt;/a&gt;.
Only a few units were produced, and the exact specifications and release dates are hard to find.&lt;/p&gt;

&lt;p&gt;Now, after all those fossilised giants, let’s finally proceed to recording and digital sound-design on desktop(-ish) computers.&lt;/p&gt;

&lt;h4 id=&quot;32-digidesign-turbosynth-1988&quot;&gt;3.2. Digidesign Turbosynth (~1988)&lt;/h4&gt;

&lt;p&gt;Digidesign, you’re gonna hear this name a lot, was a company, that was founded in 1984, and was one of the pioneers of digital audio. Now it is known as AVID with their flagship product being ProTools, which many of you probably heard of.
This company was one of the first companies to develop a digital audio workstation (DAW) for Apple Macintosh computers.
But, apparently, in parallel they developed a software called &lt;a href=&quot;http://www.muzines.co.uk/articles/digidesign-turbosynth/4028&quot;&gt;Turbosynth&lt;/a&gt;, which was a software synthesizer, that utilised the modular approach, and was capable of recording the output to a hard-disk of a computer. 
Not in real-time, of course – to good to be true.
We could surely declare this setup a winner, if we could move past the fact, that the hard-disk capacity of Macintosh II, which was the most powerful Macintosh computer at that time, was around 40MB (the largest storage peripheral you could get), which is a hard-shot for storing digital recording that has a fidelity tolerable for semi/professional audio productions.
There was also an Atari ST version. Atari ST had a hard-disk add-on called &lt;a href=&quot;https://www.computinghistory.org.uk/det/48870/Atari-Megafile-60/&quot;&gt;Megafile 60&lt;/a&gt;, which had a capacity of 60MB. Slightly better, however it was launched in 1990, which is a bit late for our story.&lt;/p&gt;

&lt;h4 id=&quot;33-digidesign-sound-tools-1989&quot;&gt;3.3. Digidesign Sound Tools (1989)&lt;/h4&gt;
&lt;p&gt;In 1989 Digidesign launched a product called &lt;a href=&quot;https://www.soundonsound.com/reviews/digidesign-sound-tools&quot;&gt;Sound Tools&lt;/a&gt;, which could be considered a predecessor of what we traditionally call a DAW. 
This was a hybrid system. The hardware part was a 2-channel 16-bit A/D converter, which was connected to a Macintosh computer via SCSI interface. The software was a set of tools for audio recording and editing, familiar to anyone who ever used a computer for audio production.
Sound Tools was proudly marketed as “tapeless recording technology”, and was a direct competitor to the aforementioned DAT format.
Usually, when talking about the “first digital recording” this is the only thing mentioned. 
However, there is one more thing, that I would like to tell you about.&lt;/p&gt;

&lt;h4 id=&quot;34-maxmspfts-1989&quot;&gt;3.4. Max/&lt;del&gt;MSP&lt;/del&gt;FTS! (1989)&lt;/h4&gt;
&lt;p&gt;According to &lt;a href=&quot;https://en.wikipedia.org/wiki/Max_(software)&quot;&gt;wikipedia&lt;/a&gt;, In 1989, IRCAM developed Max/FTS (“Faster Than Sound”), a version of Max ported to the IRCAM Signal Processing Workstation (ISPW) for the NeXT. Also known as “Audio Max”, it would prove a forerunner to Max’s MSP audio extensions, adding the ability to do real-time synthesis using an internal hardware digital signal processor (DSP) board. 
The &lt;a href=&quot;https://en.wikipedia.org/wiki/ISPW&quot;&gt;machine&lt;/a&gt; was based on my beloved Next Computer, I would love this thing to be actually declared a winner of this DSP-arms-race. Sadly, the cost DSP extension board alone was approximately $12,000 US, which made it prohibitively expensive outside of research institutes and universities.&lt;/p&gt;

</description>
        <pubDate>Sat, 03 Jun 2023 11:17:57 +0300</pubDate>
        <link>/2023/History_of_Digital_Recording/</link>
        <guid isPermaLink="true">/2023/History_of_Digital_Recording/</guid>
        
        <category>dsp</category>
        
        
        <category>dsp audio software</category>
        
        <category>music</category>
        
        <category>journalism?</category>
        
      </item>
    
      <item>
        <title>Live Bounce Recording Using Terminal</title>
        <description>&lt;h4 id=&quot;icebreaker&quot;&gt;Icebreaker&lt;/h4&gt;
&lt;p&gt;My obsession with using found sounds in my recording, often gives me a sudden itch to bounce a few seconds of audio output from a random application on my computer. 
Say, a web application or a video player.&lt;/p&gt;

&lt;p&gt;And then I really feel, how cumbersome and inflated it is to start a general purpose DAW for that, and crawl through an 
awkward ritual of gently rubbing its multichannel mixer and transport controls for that. It is the same as 
summoning a 12 feet tall four-horned purple demon to open a can of beans for you.&lt;/p&gt;

&lt;p&gt;There are of course lightweight open source wave editors, like &lt;a href=&quot;https://www.audacityteam.org/&quot;&gt;Audacity&lt;/a&gt;.&lt;br /&gt;
Sadly, previous versions of it had some problems with recording “virtual” audio outputs akin to &lt;a href=&quot;https://existential.audio/blackhole/&quot;&gt;BlackHole 16ch&lt;/a&gt;, 
I’m currently using.&lt;/p&gt;

&lt;p&gt;Seems to be working correctly from version 3.2.5 onwards, though. Till this version was published I had plenty of free time to develop my own crappy solution
for yet another purposely invented problem. But this is how you mine proper tech-blog material, isn’t it? So, let’s go!&lt;/p&gt;

&lt;h4 id=&quot;the-ingredients&quot;&gt;The Ingredients&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://sox.sourceforge.net/&quot;&gt;SoX&lt;/a&gt; - swiss army knife for DSP via terminal, as it is nominated by its developers. And, oh boy, indeed it is.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ffmpeg.org/ffplay.html&quot;&gt;ffplay&lt;/a&gt; - a part of FFmpeg audio suite, and it is a very simple portable media player using the FFmpeg libraries&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;the-magic&quot;&gt;The Magic&lt;/h4&gt;

&lt;p&gt;UNIX-like terminal command piping using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;|&lt;/code&gt; operator. Pipe operator placed between two commands simply tells the shell,
that first commands input becomes second command output. This thing combined with another UNIX paradigm called &lt;a href=&quot;https://en.wikipedia.org/wiki/Redirect_(computing)&quot;&gt;IO Redirect&lt;/a&gt;
is what really unlocks that “shell magic” at the reach of your fingertips (unless half of them had forever ingrown into
that horrible bio-prosthetic named after a small rodent species). So the commands for bouncing live output to a file are the following:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sox &lt;span class=&quot;nt&quot;&gt;-V6&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; coreaudio &lt;span class=&quot;s2&quot;&gt;&quot;BlackHole 16ch&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; wav - | ffplay -
sox &lt;span class=&quot;nt&quot;&gt;-V6&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; coreaudio &lt;span class=&quot;s2&quot;&gt;&quot;BlackHole 16ch&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; wav output.wav
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;As you can see, I provided two very similar commands here. Indeed, I use the same source for both of them, which is my 
virtual input device. 
First command just redirects the output of my virtual device to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ffplay&lt;/code&gt;. This solves the recording monitoring problem. 
Next one actually does the recording of audio to disk. 
Monitoring pipeline sometimes fails, while recording is stable, so I would recommend to run it in separate terminal 
sessions.&lt;/p&gt;

&lt;h4 id=&quot;but-wait-there-is-more&quot;&gt;But wait, there is more!&lt;/h4&gt;

&lt;p&gt;Albeit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sox&lt;/code&gt; does not look all that “user-friendly”, it can be much smarter than trivial methods of live record bouncing. Especially, when it comes to trimming silence.
Let’s consider the following command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sox &lt;span class=&quot;nt&quot;&gt;-V6&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; coreaudio &lt;span class=&quot;s2&quot;&gt;&quot;BlackHole 16ch&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; wav output.wav silence 1 0.1 1% 1 0.5 1%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This command will record audio from my virtual device, and trim silence from the beginning and the end of the recording. Technically, this applies “silence” effect as a part the internal FX chain of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sox&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Let’s break down the parameters a bit:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Parameter&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1 0.1 1%&lt;/td&gt;
      &lt;td&gt;Detects silence segments that are at least 1 second long, with a silence threshold of 0.1 (10% of maximum volume).&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1 0.5 1%&lt;/td&gt;
      &lt;td&gt;Removes silence segments that are at least 1 second long, with a silence threshold of 0.5 (50% of maximum volume).&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;You can read more about &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sox&lt;/code&gt; silence effect &lt;a href=&quot;https://sox.sourceforge.io/sox.html#EFFECTS&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When bouncing live audio, you would probably want some safeguard to prevent overwriting existing files. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sox&lt;/code&gt; does not have a built-in solution for that, but you can easily write a small shell script to do that for you. Here is a small example:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;wildcard&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;counter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1

&lt;span class=&quot;k&quot;&gt;while &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;wildcard&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;/\*/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$counter&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$filename&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then
&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$filename&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;break
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;fi&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;counter++&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Then you can use it like this:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;output_filename&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;./find_next_file.sh &lt;span class=&quot;s2&quot;&gt;&quot;output*.wav&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
sox &lt;span class=&quot;nt&quot;&gt;-V6&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; coreaudio &lt;span class=&quot;s2&quot;&gt;&quot;BlackHole 16ch&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; wav &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$output_filename&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;!-- readmore --&gt;

&lt;h4 id=&quot;additional-notes&quot;&gt;Additional Notes&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sox&lt;/code&gt; will record all the available input channels of your device into a single file. In my case it is 16 channels. How often do you deal with 16-channel WAV files? I was personally quite surprised that standard WAVEs support more than 2 channels. Indeed, it was a relatively recent addition to the standard.
The WAVE format began supporting more than two channels with the introduction of the WAVE Format Extensible (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WAVEFORMATEXTENSIBLE&lt;/code&gt;) specification.&lt;/p&gt;

&lt;p&gt;This specification was introduced as part of the DirectX Media Objects (DMO) architecture, which was included with DirectX 8.0, released by Microsoft in 2000.
It is worth noting, that this specification was most likely designed to support spatial audio, and not multi-track recording.&lt;/p&gt;

&lt;p&gt;However, the spatilisation mapping is not a part of the WAVE specification, and is defined by the application. There are several different spatialisation schemes, and they are not compatible with each other. So, the exact spatialisation you get will depend of the software.&lt;/p&gt;

&lt;p&gt;I was wondering, why this problem was not addressed by the BWF convention, which adds broadcasting-specific metadata to the WAVE format, such as timecodes and project information. Probably, because BWF was introduced in 1997, three years prior to the WAVEFORMATEXTENSIBLE specification.&lt;/p&gt;

&lt;p&gt;It is a bit of a shame, because it would be a perfect place to store the channel mapping information. However, there is a &lt;a href=&quot;https://tech.ebu.ch/docs/tech/tech3285.pdf&quot;&gt;proposal&lt;/a&gt; and also, a &lt;a href=&quot;https://tech.ebu.ch/docs/r/r111.pdf&quot;&gt;recommendation&lt;/a&gt; to extend the BWF specification to support multi-channel audio. It is not a part of the official BWF specification yet, but it is already supported by some software, like &lt;a href=&quot;https://www.sounddevices.com/wave-agent&quot;&gt;Sound Devices Wave Agent&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Max MSP is a very powerful tool for audio processing, and it can be used for live audio recording as well. However, it is not a lightweight solution for that by any means, despite it does not have the overhead of conventional linear multi-track audio edtitor. For bouncing live audio using Max there is a very convenient  &lt;a href=&quot;https://docs.cycling74.com/max5/refpages/msp-ref/sfrecord~.html&quot;&gt;~sfrecord&lt;/a&gt; object.&lt;/p&gt;

&lt;p&gt;Seems like this &lt;a href=&quot;https://madskjeldgaard.dk/posts/sox-tutorial-cli-tape-music/#:~:text=SoX%20includes%20a%20very%20handy,c%20in%20the%20terminal%20window.&quot;&gt;tech-blog&lt;/a&gt; does a much better job at breaking down the capabilities of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sox&lt;/code&gt; than me.&lt;/p&gt;
</description>
        <pubDate>Sat, 13 May 2023 12:05:05 +0300</pubDate>
        <link>/2023/Live_Bounce_Recording_Using_Terminal/</link>
        <guid isPermaLink="true">/2023/Live_Bounce_Recording_Using_Terminal/</guid>
        
        <category>bash</category>
        
        <category>dsp</category>
        
        
        <category>vst audio software</category>
        
      </item>
    
      <item>
        <title>Docker Audio Hell Pt. 2</title>
        <description>&lt;h4 id=&quot;false-promises-of-posix-compatibility&quot;&gt;False Promises of POSIX-compatibility&lt;/h4&gt;

&lt;p&gt;In &lt;a href=&quot;/2022/Docker_Audio_Hell/&quot;&gt;previous article&lt;/a&gt; we briefly touched upon evolution of audio playback and processing under Unix-like operating system. That was quite a ride, if you remember. Maintaining traditions firmly set in this blog, let’s ask ourselves a question, how can we make my experience with *NIX audio even more pathological. Any suggestions? Right! Let’s run it in Docker under MacOS.&lt;/p&gt;

&lt;p&gt;Ok. Let’s first politely (for the most part) dismiss the obvious suggestions from the sane-minded people. This would be sure mixed with some old-man’s rumbling touching the peculiarity of his ill habits.
Q: Why not just install Wine on your host system? 
A: Being a mentally challenged paranoid I am, I have to state that I\m simply afraid. As of now, MacOS has completely dropped 32-bit support, the instructions of building and setting up Wine on those kind of systems looks like for building a portal to Hell. I’m not sure if I’m ready for that yet.&lt;/p&gt;

&lt;p&gt;Q: Why not use a proper HyperVisor, instead of crippled Docker, which was not designed for those kinds of shenanigans anyway?
A: Two point here. First, I simply do not want to install another piece of heavyweight software on my laptop (which is, technically, not even mine). Haven’t I already have this Docker thing lying around and my dayjob makes me interact with it a lot. So why not utilise it for something useful? (depends, on how one defines “useful”, of course)&lt;/p&gt;

&lt;p&gt;Q: What’s the use of some old Win32 application?  It does not fit the modern audio production standards at all. 
A: That’s just how I roll, sir/ma’m&lt;/p&gt;

&lt;p&gt;On normal Unix-like systems, the host’s sound (and MIDI) hardware could be simply shared via plain docker device mountpoint, like:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# x11docker provides this setup with option --alsa.&lt;/span&gt;
docker run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--device&lt;/span&gt; /dev/snd ALSAIMAGE speaker-test
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, on our beloved slick 2+K$ tinfoil cans, we are going into…&lt;/p&gt;

&lt;!-- readmore --&gt;

&lt;h4 id=&quot;hoops-of-hellfire-and-blades&quot;&gt;Hoops of Hellfire and Blades&lt;/h4&gt;

&lt;p&gt;As an alternatively-abled, alternative OS (MacOS X) user, I do not have this luxury. The solution proposed, was to actually utilise pulseaudio native audio-over-the-network support. As I described earlier, conceptually it works, but I spotted two problems with that:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Some strange latency issues, latency increases over time, and requires me to periodically restart the server on my host OS.&lt;/li&gt;
  &lt;li&gt;My version of pulseaudio, obtained from Homebrew (I believe), does not allow me to output to a virtual device (in my case, BlackHole 16ch)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Seems, like the problem here is no particular device being virtual, but rather PulseAudio failing at creating sink name. See https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/User/SupportedAudioFormats/ . I could try to debug this, however, my attempts at building latest pulseaudio could be compared to monkey trying to land a fighter jet (no offence to the monkeys, through, I managed to build it, but then it failed to start, stating that my alternative OS semaphores are not POSIX compatible).&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;MacOS PulseAudio Server Logs
&lt;/summary&gt;
  &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;I: [] module-coreaudio-device.c: Initializing module for CoreAudio device 'BlackHole 16ch' (id 52)
I: [] module-card-restore.c: Restoring port latency offsets for card BlackHole_16ch.
D: [] card.c: Looking for initial profile for card BlackHole_16ch
D: [] card.c: on availability unknown
I: [] card.c: BlackHole_16ch: active_profile: on
I: [] card.c: Created 0 &quot;BlackHole_16ch&quot;
D: [] module-coreaudio-device.c: Sample rate: 44100.000000
D: [] module-coreaudio-device.c: 64 bytes per packet
D: [] module-coreaudio-device.c: 1 frames per packet
D: [] module-coreaudio-device.c: 64 bytes per frame
D: [] module-coreaudio-device.c: 16 channels per frame
D: [] module-coreaudio-device.c: 32 bits per channel
D: [] module-coreaudio-device.c: Stream name is &amp;gt;Channel 1, Channel 2, Channel 3, Channel 4, Channel 5, Channel 6, Channel 7, Channel 8, Channel 9, Channel 10, Channel 11, Channel 12, Channel 13, Channel 14, Channel 15, Channel 16&amp;lt;
D: [] module-device-restore.c: Database contains no data for key: sink:Channel_1__Channel_2__Channel_3__Channel_4__Channel_5__Channel_6__Channel_7__Channel_8__Channel_9__Channel_10__Channel_11__Chann
D: [] module-device-restore.c: Database contains no (or invalid) data for key: sink:Channel_1__Channel_2__Channel_3__Channel_4__Channel_5__Channel_6__Channel_7__Channel_8__Channel_9__Channel_10__Channel_11__Chann:null
I: [] sink.c: Created sink 0 &quot;Channel_1__Channel_2__Channel_3__Channel_4__Channel_5__Channel_6__Channel_7__Channel_8__Channel_9__Channel_10__Channel_11__Chann&quot; with sample spec float32le 16ch 44100Hz and channel map mono,mono,mono,mono,mono,mono,mono,mono,mono,mono,mono,mono,mono,mono,mono,mono
I: [] sink.c:     device.string = &quot;BlackHole 16ch&quot;
I: [] sink.c:     device.product.name = &quot;BlackHole 16ch&quot;
I: [] sink.c:     device.description = &quot;BlackHole 16ch&quot;
I: [] sink.c:     device.access_mode = &quot;mmap&quot;
I: [] sink.c:     device.class = &quot;sound&quot;
I: [] sink.c:     device.api = &quot;CoreAudio&quot;
I: [] sink.c:     device.buffering.buffer_size = &quot;32768&quot;
I: [] sink.c:     device.vendor.name = &quot;Existential Audio Inc.&quot;
I: [] sink.c:     device.icon_name = &quot;audio-card&quot;
D: [] source.c: Failed to register name Channel_1__Channel_2__Channel_3__Channel_4__Channel_5__Channel_6__Channel_7__Channel_8__Channel_9__Channel_10__Channel_11__Chann.monitor.
I: [] sink.c: Freeing sink 0 &quot;Channel_1__Channel_2__Channel_3__Channel_4__Channel_5__Channel_6__Channel_7__Channel_8__Channel_9__Channel_10__Channel_11__Chann&quot;
E: [] module-coreaudio-device.c: unable to create sink.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;Those couple of issues mentioned above, of course, make recording the container’s audio output comparable to jumping through the hoops of hellfire and blades every tine. So, instead of that, I decided to use 
OulseAudio RTP sink support, which could be defined like that (on the host system):&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pulseaudio &lt;span class=&quot;nt&quot;&gt;--load&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;module-native-protocol-tcp &lt;span class=&quot;nt&quot;&gt;--exit-idle-time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-1&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-vvvvv&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# since we did not daemonize the serivce, the commands below can be executed in a separate terminal window&lt;/span&gt;
pacmd load-module module-null-sink &lt;span class=&quot;nv&quot;&gt;sink_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rtp
pacmd load-module module-rtp-send &lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rtp.monitor &lt;span class=&quot;nv&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1234 &lt;span class=&quot;nv&quot;&gt;loop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Real-time_Transport_Protocol&quot;&gt;RTP protocol&lt;/a&gt; can work in two modes, standard single-cast and (multi-cast)[https://en.wikipedia.org/wiki/Real-time_Transport_Protocol]. So, if use define a sink with default options, . And unfortunately, you would never guess, what would be the mutkcast address, you’ll have to probe it manually, or grep pulseaudio logs. And this option only accetps single IP or multi-cast range, no hostname resolve supported. So, the “docker.io” tricks would not work here (the phrase “would not work” could be the most frequent word combination of this blog, need to check for that one day).&lt;/p&gt;

&lt;p&gt;By the way, you can check the UDP communication between docker container and host using netcat.&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# inside the container&lt;/span&gt;
nc &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; &amp;lt;HOST_IP&amp;gt; 1234

&lt;span class=&quot;c&quot;&gt;# on host machine&lt;/span&gt;
nc &lt;span class=&quot;nt&quot;&gt;-ul&lt;/span&gt; 1234 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And we are also have to do some exotic magic to simply obtain the host’s IP on the Docker network. Our life would be too easy and dull without that, wouldn’t it?&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# either an arbitrary IPv4 address from host&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;Hostip&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;ip &lt;span class=&quot;nt&quot;&gt;-4&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; a | &lt;span class=&quot;nb&quot;&gt;awk&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'{print $4}'&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;cut&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt;/ &lt;span class=&quot;nt&quot;&gt;-f1&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; 127.0.0.1 | &lt;span class=&quot;nb&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# or especially IP from docker daemon&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;Hostip&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;ip &lt;span class=&quot;nt&quot;&gt;-4&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; a| &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;docker0 | &lt;span class=&quot;nb&quot;&gt;awk&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'{print $4}'&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;cut&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt;/ &lt;span class=&quot;nt&quot;&gt;-f1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then why not simply define a loop back address and a free UDP port, by convention for RTP it’s 1234 (RTP usually uses UDP as a transport). Then we could simply forward this port from guest to host. Well, shocking news, ladies ang gentelmen, 
in Docker world, ports are forwared &lt;em&gt;from host to container&lt;/em&gt;, not the other way around. So, if you are into that kind of naughty stuff, you\ll have to use dockers’s –network=host option. So, then you can determine the host’s IP and speicify it in the config. (Hack me gently with a chainsaw, but I do not remember, why loopback did not work in this case).&lt;/p&gt;

&lt;h4 id=&quot;6inks-6inks-6inks&quot;&gt;6inks, 6inks, 6inks&lt;/h4&gt;

&lt;p&gt;Aaaaaand, it works. Well, sort of. I was able to pass my audio to the &lt;em&gt;other side&lt;/em&gt;. It was accompanied by a constant wall of digital noise, though. 
When I try to stream raw PCM audio using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;faplay&lt;/code&gt;, for example:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ffplay  &lt;span class=&quot;nt&quot;&gt;-sample_rate&lt;/span&gt; 44100 &lt;span class=&quot;nt&quot;&gt;-autoexit&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; s16be rtp://127.0.0.1:1234
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;When I try to do the same, with, say, VLC player, it tries to interpret the data stream as some interleaved AV, and outputs all kind of glitchy weirdess in the video window.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vlc &lt;span class=&quot;nt&quot;&gt;--demux&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rawaud &lt;span class=&quot;nt&quot;&gt;--rawaud-channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2 &lt;span class=&quot;nt&quot;&gt;--rawaud-samplerate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;44100 udp://@:1234
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;First, I thought it was raw audio byte format problem, judging by the logs the defaul format is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s16be&lt;/code&gt; (16 bit big-endian). Just to get gist of the lunacy factor, you can take a tour of audio byte format zoo &lt;a href=&quot;https://www.freedesktop.org/wiki/Software/PulseAudio/Documentation/User/SupportedAudioFormats/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;However, I made sure they are matching and it left me with nothing but two more flavoirs of digital audio pollution. Well, it’s time to reveal you the brutal truth. Pulseaudio RTP sinks are &lt;em&gt;multi-cast only&lt;/em&gt;. So, when you specify a single IP address and port there, it dumps all the it’s sister RTP Control Stuff (stats, metadata and heartbeats) right above the raw audio, manifestating itself in a form a constant audiable noise.&lt;/p&gt;

&lt;p&gt;So, I swithed to multicast, and now now I’m able to get the audio output from Docker container to my virtual device for recording. 
The latency is still there of course. Thankfully, I’m not obligated to perform live with this kind of setup.&lt;/p&gt;

&lt;h4 id=&quot;i-might-still-be-insane-though&quot;&gt;I might still be insane, though&lt;/h4&gt;

&lt;p&gt;PulseAudio has native mechanism for capturing audio. The monitor sink is created the similar way, and then, allegedly, you can pipe it anywhere.&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pactl load-module module-null-sink &lt;span class=&quot;nv&quot;&gt;sink_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;steam
parec &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; steam.monitor | oggenc &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt; 192 &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; steam.ogg &lt;span class=&quot;nt&quot;&gt;--raw&lt;/span&gt; - 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;oggenc&lt;/code&gt; is just for example here. Raw stream can be encoded into anything. The thing I have not tested yet, is would the standard output still be available, as recording without monitoring is not fun at all. Trust me, I used to do that.&lt;/p&gt;

</description>
        <pubDate>Sun, 21 Aug 2022 17:26:57 +0300</pubDate>
        <link>/2022/Docker_Audio_Hell_Pt__2/</link>
        <guid isPermaLink="true">/2022/Docker_Audio_Hell_Pt__2/</guid>
        
        <category>docker</category>
        
        <category>wine</category>
        
        <category>pulseaudio</category>
        
        
        <category>vst audio software</category>
        
        <category>dsp</category>
        
      </item>
    
      <item>
        <title>Cursed FM Synth</title>
        <description>&lt;h4 id=&quot;as-it-was-not-cursed-enough-already&quot;&gt;As it was not cursed enough already&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cursedfm7.svg&quot; alt=&quot;heck&quot; width=&quot;100%&quot; style=&quot;margin-top: 20px;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;audio-container&quot; style=&quot;margin-bottom: 32px;&quot;&gt;
	&lt;audio src=&quot;/assets//mp3/2022/cursedfm.mp3&quot; preload=&quot;auto&quot;&gt;&lt;/audio&gt;
&lt;/div&gt;

&lt;p&gt;You all know &lt;a href=&quot;https://asb2m10.github.io/dexed/&quot;&gt;DEXED&lt;/a&gt;, an awesome Yamaha DX7 librarian. Seems like I stumbled upon an undocumented feature. When you use it to load a DX7 cart dump, it allows you to select any file on your system. So, why not pick a file, that has nothing to do with DX or even music whatsoever? This way it fills all the registers of a virtual Yamaha with rubbish. And it sounds wild, folks, absolutely untamed.&lt;/p&gt;

&lt;p&gt;Listening back to this, I was kinda imagining those big heads like B. Eno or T. Reznor who have somehow found a way, to do the same with a real physical DX7 keyvoard. Was it really  a dedicated programmator, or were they just sending digital noise in form of SYSEX  messages?&lt;/p&gt;

&lt;p&gt;Probably the former, no matter what my corrupted imagkintation tells me.&lt;/p&gt;

&lt;div class=&quot;downloadBox&quot; onclick=&quot;window.location='/assets/CURSED.syx'; return false;&quot;&gt;
  Download SYX&lt;br /&gt;
  &lt;p style=&quot;font-size: small; margin-top: 2px; margin-bottom: 2px;&quot;&gt;Cursed DX7 Cart&lt;/p&gt;
&lt;/div&gt;

</description>
        <pubDate>Sun, 26 Jun 2022 14:38:14 +0300</pubDate>
        <link>/2022/Cursed_FM_Synth/</link>
        <guid isPermaLink="true">/2022/Cursed_FM_Synth/</guid>
        
        
      </item>
    
      <item>
        <title>OverType TypeWriter Simulator</title>
        <description>&lt;h4 id=&quot;when-too-realistic-is-not-enough&quot;&gt;When Too Realistic is Not Enough&lt;/h4&gt;

&lt;p&gt;Have you folks seen this amazing JS demo, resembling an &lt;a href=&quot;https://uniqcode.com/typewriter/&quot;&gt;old typewriter&lt;/a&gt;? I, personally, find it hilarious and enjoyable. The ink refill process, the brokeness fader, the click sound, this makes it  almost an ideal &lt;em&gt;soft machine&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;There is one  problem though, which, sadly, cannot really attribute that to the precision of the simulation. Sometimes, it will just hangs up, stop reacting to any keystrokes, and there is no way to save the text you’ve already typed, or, continuing the analogy, - feed the page back into the roll, after you fix the jam.&lt;/p&gt;

&lt;p&gt;My intuitive approach to this show-stopper, would be just re-run the bits of javascript that set the virtual typewriter up. I found a very useful code bit somewhere, which worked exactly as I expected.&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;getElementsByTagName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;innerHTML&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//run script inside div&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thanks to this, I was able to finish a page of my upcoming horror-story fanzine. Hell yeah, you cannot really achieve it in any other way, than using a good old typewriter!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/fanzine01_thumb.png&quot; data-ngsrc=&quot;/assets/images/fanzine01.jpg&quot; data-nanogallery2-lightbox=&quot;{ &amp;quot;viewerGallery &amp;quot;: &amp;quot;none&amp;quot; }&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Jun 2022 13:50:55 +0300</pubDate>
        <link>/2022/OverType_TypeWriter_Simulator/</link>
        <guid isPermaLink="true">/2022/OverType_TypeWriter_Simulator/</guid>
        
        <category>js</category>
        
        
        <category>code bits</category>
        
        <category>journalism?</category>
        
      </item>
    
      <item>
        <title>Docker Audio Hell Pt. 1</title>
        <description>&lt;h4 id=&quot;and-the-passive-amoeba-of-linux-audio&quot;&gt;And The Passive Amoeba of Linux Audio&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/figures/amoeba1.svg&quot; alt=&quot;heck&quot; width=&quot;100%&quot; style=&quot;margin-top: 20px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this article, we are going to quickly reiterate the reason why audio-production on Linux can be challenging (if not frustrating at times). I think, the main reason for that, besides of course, my own incompetence, is audio-facilties being handled in too much of Unix-way. But what is Unix way, anyway? Well, in short, we have one isolated piece of software doing one single thing, but doing well.If any additional processing needs to be done with the output, we pipe (|) it into another isolated piece of software. Several steps of that form a &lt;em&gt;pipeline&lt;/em&gt;. 
Sounds great, right? Well, unfortunately, sometimes the seeming tidiness of this approach could quikcly go down the tubes (pun intended). 
Once you need something specific, or something simply done &lt;em&gt;your&lt;/em&gt; way, you’ll have to explore the whole pipeline from end to end. It’s alright, if the nodes there are simple utilities like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt;, but imaging exploring each of those having massive tectonic layers of legacy software archeology above them.&lt;/p&gt;

&lt;p&gt;So, here is how this usually happens. It all starts with ALSA, a kernel subsystem, which works directly with your audio hardware. It does not efficiently handle software mixing and routing, though, so applications  cannot share your device inputs/outputs effieicently, and we were not even talking about routing audio inputs and outputs between them. 
To satisfy those needs, PulseAudio was introduced. A software wrapper around ALSA, that re-routes all the audio streams through itself, and distributes it between existing hardware inputs and outputs (sinks). Applications compiled to use PulseAuido as audio-driver cannot use ALSA directly, all according to the hightest standards of incomprehensible madman’s logic we maintain in the world of prograaming.&lt;/p&gt;

&lt;p&gt;PulseAudio, however had some issues with latency, which made it hardly usable for “professional” audio-recording, that usually includes near real-time record monitoring, for example. 
To solve problem, purposedly introdued, we wrote an alternative to PulseAudio called JACK. Does this remind you of &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Futurological_Congress&quot;&gt;“The Futurological Congress”&lt;/a&gt;? For me it totally does.
For a long time, JACK was the standard for (semi)professional audio work on Linux. Apparently, does not play nicely with PulseAudio. Some applications, (ex: Wine), does not support JACK at all. Of course, Wine has WineASIO, which can be routed to JACK, however, it is possible only for applications that use ASIO for audio, i.e. DAWs.&lt;/p&gt;

&lt;p&gt;Both JACK and Pulseaudio, have different solutions for audio-over network. PulseAudio has native client-server support, and also , a special audio-sink for streaming audio in RTP protocol (is it the same thing as zeroconf or not?). Judging by these configuration strings, client-server support of pulseaudio, can use TCP as a transport, and something called “native”, which I presume, would be standard unix-sockets.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/figures/amoeba2.svg&quot; alt=&quot;heck&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;JACK provides &lt;a href=&quot;https://jackaudio.org/faq/netjack.html&quot;&gt;several ways&lt;/a&gt; remote audio networking. It’s native &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;netone&lt;/code&gt; addon using CELT codec and master/follower pattern,  and some newer netJACK2, which has network discovery.&lt;/p&gt;

&lt;p&gt;Lately, yet another ultra-low-latency professional-audio-grade Unix soundsystem was introduced. It’s called PipeWire, and it is sort of a chamenion protocol, which can act as a PulseAudio backend for PulseAudio clients, Jack backend to Jack clients and so on. AFAIK, Pipewire does not have audio-over-network support.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Some amoebas are not shown on the figures, simply because they are considered extinct. Before ALSA, earlier versions of Linux were shipped with Open Sound System (OSS), which then was appropriated and no longer counts as free software. However, as we know, once introduced at the kernel level, is destined to be supported for eternity. That’s why ALSA still has an OSS emulation mode.&lt;/p&gt;
</description>
        <pubDate>Tue, 07 Jun 2022 09:47:54 +0300</pubDate>
        <link>/2022/Docker_Audio_Hell/</link>
        <guid isPermaLink="true">/2022/Docker_Audio_Hell/</guid>
        
        <category>docker</category>
        
        <category>wine</category>
        
        <category>pulseaudio</category>
        
        
        <category>vst audio software</category>
        
        <category>dsp</category>
        
      </item>
    
  </channel>
</rss>
